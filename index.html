<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Audio Stream with Visualization</title>
    <style>
      canvas {
        display: block;
        width: 100%;
        height: 150px;
        background-color: #000;
      }
    </style>
  </head>
  <body>
    <h1>Audio Stream</h1>
    <audio id="audioPlayer" controls autoplay></audio>
    <canvas id="visualizer"></canvas>
    <script>
      const audio = document.getElementById("audioPlayer");
      const canvas = document.getElementById("visualizer");
      const canvasCtx = canvas.getContext("2d");
      const mediaSource = new MediaSource();
      audio.src = URL.createObjectURL(mediaSource);

      mediaSource.addEventListener("sourceopen", () => {
        const sourceBuffer = mediaSource.addSourceBuffer("audio/mpeg");
        fetchStream(sourceBuffer);
      });

      async function fetchStream(sourceBuffer) {
        const response = await fetch("/stream");
        const reader = response.body.getReader();

        const readChunk = async () => {
          const { done, value } = await reader.read();
          if (done) {
            mediaSource.endOfStream();
            return;
          }
          sourceBuffer.appendBuffer(value);
          await updateEnd();
          readChunk();
        };

        const updateEnd = () => {
          return new Promise((resolve) => {
            sourceBuffer.addEventListener("updateend", resolve, { once: true });
          });
        };

        readChunk();
      }

      // Start audio context immediately after creation
      const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      audioCtx
        .resume()
        .then(() => {
          console.log("Audio context started");
        })
        .catch((err) => {
          console.error("Failed to start audio context:", err);
        });

      // Set up Web Audio API context and nodes for visualization
      const analyser = audioCtx.createAnalyser();
      const source = audioCtx.createMediaElementSource(audio);
      source.connect(analyser);
      analyser.connect(audioCtx.destination);
      analyser.fftSize = 2048;
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);

      // Draw visualization
      function draw() {
        requestAnimationFrame(draw);

        analyser.getByteTimeDomainData(dataArray);

        canvasCtx.fillStyle = "rgb(0, 0, 0)";
        canvasCtx.fillRect(0, 0, canvas.width, canvas.height);

        canvasCtx.lineWidth = 2;
        canvasCtx.strokeStyle = "rgb(0, 255, 0)";

        canvasCtx.beginPath();

        const sliceWidth = (canvas.width * 1.0) / bufferLength;
        let x = 0;

        for (let i = 0; i < bufferLength; i++) {
          const v = dataArray[i] / 128.0;
          const y = (v * canvas.height) / 2;

          if (i === 0) {
            canvasCtx.moveTo(x, y);
          } else {
            canvasCtx.lineTo(x, y);
          }

          x += sliceWidth;
        }

        canvasCtx.lineTo(canvas.width, canvas.height / 2);
        canvasCtx.stroke();
      }

      draw();

      // Start audio context on user interaction (required by some browsers)
      document.body.addEventListener("click", () => {
        if (audioCtx.state === "suspended") {
          audioCtx.resume();
        }
      });

      canvas.click();
      audio.play();
    </script>
  </body>
</html>
